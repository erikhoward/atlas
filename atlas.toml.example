# ============================================================================
# Atlas Configuration Example
# ============================================================================
# Copy this file to atlas.toml and customize for your environment
#
# Atlas supports two database backends:
#   - Azure Cosmos DB (NoSQL, globally distributed)
#   - PostgreSQL 14+ (Relational, JSONB support)
#
# Choose your backend by setting export.database_target below.
#
# For detailed configuration options, see:
#   - docs/configuration.md - Complete configuration reference
#   - docs/postgresql-setup.md - PostgreSQL setup guide
#   - examples/atlas.example.toml - CosmosDB example
#   - examples/atlas.postgresql.example.toml - PostgreSQL example
# ============================================================================

[application]
name = "atlas"
version = "1.0.0"
log_level = "info"  # trace, debug, info, warn, error
dry_run = false

[openehr]
# EHRBase server endpoint
base_url = "https://ehrbase.example.com/ehrbase/rest/openehr/v1"
vendor = "ehrbase"  # Vendor-specific implementation

# Authentication
auth_type = "basic"  # basic | openid (future)
username = "atlas_user"
password = "${ATLAS_OPENEHR_PASSWORD}"  # Env var substitution

# TLS settings
tls_verify = true
# tls_ca_cert = "/path/to/ca.crt"  # Optional

[openehr.query]
# Export filters
template_ids = ["IDCR - Adverse Reaction List.v1", "IDCR - Problem List.v1"]
ehr_ids = []  # Empty = all EHRs
# time_range_start = "2024-01-01T00:00:00Z"  # ISO 8601 or null
# time_range_end = null  # null = now

# Batch processing
batch_size = 1000  # 100-5000
parallel_ehrs = 8  # Concurrent EHR processing

[export]
mode = "incremental"  # full | incremental
export_composition_format = "preserve"  # preserve | flatten
database_target = "cosmosdb"  # cosmosdb | postgresql

# Retry settings
max_retries = 3
retry_backoff_ms = [1000, 2000, 4000]  # Exponential backoff

# ============================================================================
# DATABASE CONFIGURATION
# Choose ONE database backend by setting database_target above
# ============================================================================

# ----------------------------------------------------------------------------
# Option 1: Azure Cosmos DB
# ----------------------------------------------------------------------------
[cosmosdb]
# Connection
endpoint = "https://myaccount.documents.azure.com:443/"
key = "${ATLAS_COSMOS_KEY}"
database_name = "openehr_data"

# Container settings
control_container = "atlas_control"
data_container_prefix = "compositions"  # Results in: compositions_{template_id}

# Partitioning
partition_key = "/ehr_id"

# Performance
max_concurrency = 10
request_timeout_seconds = 60

# ----------------------------------------------------------------------------
# Option 2: PostgreSQL
# ----------------------------------------------------------------------------
# [postgresql]
# # Connection string format: postgresql://[user[:password]@][host][:port][/dbname][?params]
# connection_string = "postgresql://atlas_user:${ATLAS_PG_PASSWORD}@localhost:5432/openehr_data?sslmode=require"
#
# # Connection pool settings
# max_connections = 20                # Maximum connections in pool (1-100)
# connection_timeout_seconds = 30     # Timeout for acquiring connection
# statement_timeout_seconds = 60      # Timeout for SQL statement execution
#
# # SSL/TLS mode: disable | allow | prefer | require | verify-ca | verify-full
# ssl_mode = "require"                # Use 'require' or higher for production
#
# # Note: Before using PostgreSQL, run the schema migration:
# #   psql -U atlas_user -d openehr_data -f migrations/001_initial_schema.sql
# # See docs/postgresql-setup.md for detailed setup instructions

# ============================================================================
# STATE MANAGEMENT
# ============================================================================

[state]
# Watermark and checkpoint configuration for incremental exports
enable_checkpointing = true
checkpoint_interval_seconds = 30

# ============================================================================
# VERIFICATION
# ============================================================================

[verification]
# Optional data integrity verification using checksums
enable_verification = false
checksum_algorithm = "sha256"  # sha256 | sha512

# ============================================================================
# LOGGING
# ============================================================================

[logging]
# Local logging
local_enabled = true
local_path = "/var/log/atlas"
local_rotation = "daily"  # daily | size
local_max_size_mb = 100

# Azure Log Analytics (optional)
# Requires Azure AD App Registration and Data Collection Rule (DCR)
azure_enabled = false
# azure_tenant_id = "${AZURE_TENANT_ID}"
# azure_client_id = "${AZURE_CLIENT_ID}"
# azure_client_secret = "${AZURE_CLIENT_SECRET}"
# azure_log_analytics_workspace_id = "${AZURE_LOG_ANALYTICS_WORKSPACE_ID}"
# azure_dcr_immutable_id = "${AZURE_DCR_IMMUTABLE_ID}"
# azure_dce_endpoint = "${AZURE_DCE_ENDPOINT}"
# azure_stream_name = "Custom-AtlasExport_CL"

